{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "003f53f487787cd05a8bf1ce3adfa43cf19dd6ac"
      },
      "cell_type": "code",
      "source": "import re\nimport string",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train = pd.read_csv('../input/train.csv').fillna(' ')\ntest = pd.read_csv('../input/test.csv').fillna(' ')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "080cc6781b955431d304374cdd51ff568e2bb7bf"
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a0e5fc482c22d544eed1a73677e4d58d0f292205"
      },
      "cell_type": "code",
      "source": "train.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "12909c43c28b01b3cbde34c9cc9ea29250695ed1"
      },
      "cell_type": "code",
      "source": "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "08dab9792e68a0e0982ba935508f82cb4f708498"
      },
      "cell_type": "code",
      "source": "#train['none'] = 1-train[label_cols].max(axis=1)\ntrain.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e322d22fa23c4586ff0d87938ac939dba9a648c5"
      },
      "cell_type": "code",
      "source": "def prepare_for_char_n_gram(text):\n    # 1. Go to lower case (only good for english)\n    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n    clean = bytes(text.lower(), encoding=\"utf-8\")\n    clean = clean.replace(b\"\\n\", b\" \")\n    clean = clean.replace(b\"\\t\", b\" \")\n    clean = clean.replace(b\"\\b\", b\" \")\n    clean = clean.replace(b\"\\r\", b\" \")\n    \n    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n    clean = re.sub(b\"\\d+\", b\" \", clean)\n    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n    clean = re.sub(b'\\s+', b' ', clean)\n    # Remove ending space if any\n    clean = re.sub(b'\\s+$', b'', clean)\n    \n    return str(clean, 'utf-8')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "793cf28e23a3712683e7eedab248d6ab002888cb"
      },
      "cell_type": "code",
      "source": "# train[\"comment_text\"] = train[\"comment_text\"].apply(lambda x: prepare_for_char_n_gram(x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f97cbd68ceb480e4045e77a0e2c408b0ab3e8b5"
      },
      "cell_type": "code",
      "source": "# test[\"comment_text\"] = test[\"comment_text\"].apply(lambda x: prepare_for_char_n_gram(x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ae0050ffdd431beca2637c405ec3fcfdb6491b4"
      },
      "cell_type": "code",
      "source": "def count_regexp_occ(regexp=\"\", text=None):\n    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n    return len(re.findall(regexp, text))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c95d63626574a3d03ab95cce5e05ccd246b1b15"
      },
      "cell_type": "code",
      "source": "def get_indicators_and_clean_comments(df):\n    \"\"\"\n    Check all sorts of content as it may help find toxic comment\n    Though I'm not sure all of them improve scores\n    \"\"\"\n    # Count number of \\n\n    df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n    # Get length in words and characters\n    df[\"raw_word_len\"] = df[\"comment_text\"].apply(lambda x: len(x.split()))\n    df[\"raw_char_len\"] = df[\"comment_text\"].apply(lambda x: len(x))\n    # Check number of upper case, if you're angry you may write in upper case\n    df[\"nb_upper\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n    # Number of F words - f..k contains folk, fork,\n    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n    # Number of S word\n    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n    # Number of D words\n    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n    # Number of occurence of You, insulting someone usually needs someone called : you\n    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n    # Just to check you really refered to my mother ;-)\n    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n    # Just checking for toxic 19th century vocabulary\n    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n    # Some Sentences start with a <:> so it may help\n    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n    # Check for time stamp\n    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n    # Check for dates 18:44, 8 December 2010\n    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))\n    # Check for date short 8 December 2010\n    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n    # Check for http links\n    df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\n    # check for mail\n    df[\"has_mail\"] = df[\"comment_text\"].apply(\n        lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x)\n    )\n    # Looking for words surrounded by == word == or \"\"\"\" word \"\"\"\"\n    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f577cef3390dc4eca9f2a7ecde591b3e4d3fd880"
      },
      "cell_type": "code",
      "source": "get_indicators_and_clean_comments(train)\nget_indicators_and_clean_comments(test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4abb30c7256a0226c2c8549b490716a79a80c37d"
      },
      "cell_type": "code",
      "source": "num_features = [f_ for f_ in train.columns\n                    if f_ not in [\"comment_text\", \"clean_comment\", \"id\", \"remaining_chars\",\n                                      'has_ip_address'] + label_cols]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "85c4e38a89e809fe2c81379557275c528ac3856b"
      },
      "cell_type": "code",
      "source": "num_features",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4614a2cbe08aca3209290f38daf84d4842f65fd4"
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import MinMaxScaler\nfrom scipy.sparse import csr_matrix",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ac407d6f039c02d078ad66799bb3c4fefd3fc49a"
      },
      "cell_type": "code",
      "source": "skl = MinMaxScaler()\ntrain_num_features = csr_matrix(skl.fit_transform(train[num_features]))\ntest_num_features = csr_matrix(skl.fit_transform(test[num_features]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d148c0e22cafa693a20710f1d197e496b10da880"
      },
      "cell_type": "code",
      "source": "train.sample(2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cbec5be5628c8902bcee01dfa6857ae4e96c8e2a"
      },
      "cell_type": "code",
      "source": "word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    max_features=2000)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98584465bcb2e0a7a59ac642fe22a390fa09afab"
      },
      "cell_type": "code",
      "source": "trn_term_doc = word_vectorizer.fit_transform(train['comment_text'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "046fcebe94bc0c93a6df01701bc127704acf1543"
      },
      "cell_type": "code",
      "source": "test_term_doc = word_vectorizer.transform(test['comment_text'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f50f8310ac3e221f93ba769fad6b261550a738a"
      },
      "cell_type": "code",
      "source": "from scipy.sparse import hstack",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7e6b9f70783fc1dceee25ea7295486c910fc2484"
      },
      "cell_type": "code",
      "source": "csr_trn = hstack(\n            [\n                trn_term_doc,\n                train_num_features\n            ]\n        ).tocsr()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "109e29642883e19987603ecf49e2d4122f6f0821"
      },
      "cell_type": "code",
      "source": "csr_sub = hstack(\n            [\n                test_term_doc,\n                test_num_features\n            ]\n        ).tocsr()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06a1ee34ad478b192c6bbd12c1cd248bb6c148ae"
      },
      "cell_type": "code",
      "source": "Y_train=train.iloc[:,2:9].values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7446870c009e71dc50283b285c9e039651572cb0"
      },
      "cell_type": "code",
      "source": "Y_train",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5fe080021d027b0edc13cf615f4c14af62cca36d"
      },
      "cell_type": "code",
      "source": "#%%time\n#X=trn_term_doc.todense()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8fdca353b9412b634b6db42c3668647134b633c6"
      },
      "cell_type": "code",
      "source": "%%time\nX=csr_trn.todense()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6317d43e2c06f9b8f6e929bebce7b8d61509370a"
      },
      "cell_type": "code",
      "source": "#%%time\n#Y=test_term_doc.todense()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ccd496ab5937bdc74e2a1103b45d31840fd4e10b"
      },
      "cell_type": "code",
      "source": "%%time\nY=csr_sub.todense()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c8add1acb1498a2d5c36dea140cb8d35551ffc47"
      },
      "cell_type": "code",
      "source": "from xgboost import XGBRegressor",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bad64109a67d16516836f390031705668ec5c9f4",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "%%time\ndef getpredictionXGB(X,X_test):\n    my_model = XGBRegressor()\n    return my_model.fit(X, X_test, verbose=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b428932a9d0d490a1cfeb213f73e54205443d214"
      },
      "cell_type": "code",
      "source": "predi = np.zeros((len(test), len(label_cols)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "71bfb752ee8a65871491ff17d491fae1b9549a28"
      },
      "cell_type": "code",
      "source": "%%time\nfor i,j in enumerate(label_cols):\n    classifier=getpredictionXGB(X,train[j])\n    ##predi[i]=rating\n    predi[:,i]=classifier.predict(Y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "916aad2a627904f2fe27f2ec9a44d024a65ef873"
      },
      "cell_type": "code",
      "source": "submid = pd.DataFrame({'id': test[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(predi, columns = label_cols)], axis=1)\nsubmission.to_csv('submission2.csv', index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}